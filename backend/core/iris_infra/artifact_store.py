"""
Artifact storage and indexing system for Iris infrastructure.

Manages saving, indexing, and retrieving artifacts generated by tools and the agent.
"""

import asyncio
import hashlib
import json
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List
from core.utils.logger import logger
from core.iris_infra.sandbox_fs import IrisSandboxFS


class ArtifactStore:
    """
    Manages artifact storage, indexing, and retrieval.
    
    Artifacts are files generated by tools or the agent that should be
    persisted and indexed for future reference.
    """
    
    def __init__(self, iris_fs: IrisSandboxFS):
        """
        Initialize the artifact store.
        
        Args:
            iris_fs: Iris filesystem abstraction
        """
        self.iris_fs = iris_fs
        self._pending_writes: List[asyncio.Task] = []
    
    async def save_artifact(
        self,
        content: str,
        artifact_type: str,
        filename: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        blocking: bool = False
    ) -> Dict[str, Any]:
        """
        Save an artifact to the artifacts directory.
        
        Args:
            content: Artifact content to save
            artifact_type: Type of artifact (web_search, file, analysis, etc.)
            filename: Optional specific filename (auto-generated if not provided)
            metadata: Optional metadata to store with artifact
            blocking: If True, waits for write to complete; if False, writes async
            
        Returns:
            Dictionary with artifact info (path, key, metadata)
        """
        try:
            # Generate filename if not provided
            if not filename:
                timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
                content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
                filename = f"{artifact_type}_{timestamp}_{content_hash}.json"
            
            # Construct full path
            artifact_path = f"artifacts/{filename}"
            
            # Prepare artifact data
            artifact_data = {
                "content": content,
                "type": artifact_type,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "metadata": metadata or {}
            }
            
            # Write artifact
            if blocking:
                await self.iris_fs.write_json(artifact_path, artifact_data)
                logger.debug(f"Saved artifact (blocking): {artifact_path}")
            else:
                # Queue async write
                task = asyncio.create_task(
                    self._async_write_artifact(artifact_path, artifact_data)
                )
                self._pending_writes.append(task)
                logger.debug(f"Queued artifact write: {artifact_path}")
            
            # Create index entry
            index_entry = {
                "path": artifact_path,
                "type": artifact_type,
                "filename": filename,
                "tags": metadata.get("tags", []) if metadata else [],
                "created_at": artifact_data["created_at"],
                "size_estimate": len(content)
            }
            
            # Update index asynchronously
            if blocking:
                await self.iris_fs.update_file_index_entry(index_entry)
                await self.iris_fs.save_file_index()
            else:
                task = asyncio.create_task(
                    self._async_update_index(index_entry)
                )
                self._pending_writes.append(task)
            
            return {
                "artifact_path": artifact_path,
                "artifact_key": filename.replace('.json', ''),
                "type": artifact_type,
                "metadata": metadata or {}
            }
            
        except Exception as e:
            logger.error(f"Failed to save artifact: {e}", exc_info=True)
            raise
    
    async def _async_write_artifact(self, path: str, data: Dict[str, Any]) -> None:
        """Async helper for writing artifact."""
        try:
            await self.iris_fs.write_json(path, data)
            logger.debug(f"Completed async write: {path}")
        except Exception as e:
            logger.error(f"Failed async artifact write {path}: {e}")
    
    async def _async_update_index(self, entry: Dict[str, Any]) -> None:
        """Async helper for updating index."""
        try:
            await self.iris_fs.update_file_index_entry(entry)
            await self.iris_fs.flush_index_if_dirty()
            logger.debug(f"Updated index for: {entry.get('path')}")
        except Exception as e:
            logger.error(f"Failed async index update: {e}")
    
    async def get_artifact(self, artifact_key: str) -> Optional[Dict[str, Any]]:
        """
        Retrieve an artifact by its key.
        
        Args:
            artifact_key: Artifact key (filename without extension)
            
        Returns:
            Artifact data dictionary, or None if not found
        """
        try:
            # Try to find in index first
            index = await self.iris_fs.load_file_index()
            artifact_path = None
            
            for entry in index:
                if entry.get("filename", "").replace('.json', '') == artifact_key:
                    artifact_path = entry.get("path")
                    break
            
            if not artifact_path:
                # Try direct path construction as fallback
                artifact_path = f"artifacts/{artifact_key}.json"
            
            # Load artifact
            data = await self.iris_fs.read_json(artifact_path)
            return data
            
        except Exception as e:
            logger.warning(f"Failed to get artifact {artifact_key}: {e}")
            return None
    
    async def save_web_result(self, result: Dict[str, Any], query: str) -> Dict[str, Any]:
        """
        Save a web search result as an artifact.
        
        Args:
            result: Web search result dictionary
            query: Original search query
            
        Returns:
            Artifact info dictionary
        """
        metadata = {
            "query": query,
            "tags": ["web_search", "search"]
        }
        
        content = json.dumps(result, indent=2)
        return await self.save_artifact(
            content=content,
            artifact_type="web_search",
            metadata=metadata,
            blocking=False
        )
    
    async def save_task_state(self, task_data: Dict[str, Any], task_id: str) -> Dict[str, Any]:
        """
        Save task state/list as an artifact.
        
        Args:
            task_data: Task data dictionary
            task_id: Task identifier
            
        Returns:
            Artifact info dictionary
        """
        filename = f"task_state_{task_id}.json"
        metadata = {
            "task_id": task_id,
            "tags": ["task", "state"]
        }
        
        content = json.dumps(task_data, indent=2)
        return await self.save_artifact(
            content=content,
            artifact_type="task",
            filename=filename,
            metadata=metadata,
            blocking=True  # Task state should be immediately available
        )
    
    async def list_artifacts(
        self,
        artifact_type: Optional[str] = None,
        tags: Optional[List[str]] = None,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """
        List artifacts matching criteria.
        
        Args:
            artifact_type: Filter by artifact type
            tags: Filter by tags (any match)
            limit: Maximum number of results
            
        Returns:
            List of artifact index entries
        """
        try:
            index = await self.iris_fs.load_file_index()
            
            results = []
            for entry in index:
                # Filter by type
                if artifact_type and entry.get("type") != artifact_type:
                    continue
                
                # Filter by tags
                if tags:
                    entry_tags = entry.get("tags", [])
                    if not any(tag in entry_tags for tag in tags):
                        continue
                
                results.append(entry)
                
                if len(results) >= limit:
                    break
            
            # Sort by created_at (newest first)
            results.sort(key=lambda x: x.get("created_at", ""), reverse=True)
            
            return results
            
        except Exception as e:
            logger.error(f"Failed to list artifacts: {e}")
            return []
    
    async def wait_for_pending_writes(self, timeout: float = 30.0) -> None:
        """
        Wait for all pending async writes to complete.
        
        Args:
            timeout: Maximum time to wait in seconds
        """
        if not self._pending_writes:
            return
        
        try:
            logger.debug(f"Waiting for {len(self._pending_writes)} pending artifact writes")
            await asyncio.wait_for(
                asyncio.gather(*self._pending_writes, return_exceptions=True),
                timeout=timeout
            )
            self._pending_writes.clear()
            logger.debug("All pending artifact writes completed")
        except asyncio.TimeoutError:
            logger.warning(f"Timeout waiting for artifact writes after {timeout}s")
        except Exception as e:
            logger.error(f"Error waiting for pending writes: {e}")
